{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "868c1e99-5f27-4d24-98e8-3a2e9265535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BartTokenizer, BartForConditionalGeneration\n",
    "from  langchain import LLMChain, HuggingFacePipeline, PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d1cea73d-a3e2-4a5d-ad27-1c1cabdfc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('1706.03762.pdf')  #add your pdf here in the same directory.\n",
    "documents = loader.load()\n",
    "text = ''''\n",
    "текст из documents \n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d37646bb-2ba1-4064-b9ef-ff54e8f796c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10807 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# tokenize without truncation\n",
    "inputs_no_trunc = tokenizer(text, max_length=None, return_tensors='pt', truncation=False)\n",
    "\n",
    "# get batches of tokens corresponding to the exact model_max_length\n",
    "chunk_start = 0\n",
    "chunk_end = tokenizer.model_max_length  # == 1024 for Bart\n",
    "inputs_batch_lst = []\n",
    "while chunk_start <= len(inputs_no_trunc['input_ids'][0]):\n",
    "    inputs_batch = inputs_no_trunc['input_ids'][0][chunk_start:chunk_end]  # get batch of n tokens\n",
    "    inputs_batch = torch.unsqueeze(inputs_batch, 0)\n",
    "    inputs_batch_lst.append(inputs_batch)\n",
    "    chunk_start += tokenizer.model_max_length  # == 1024 for Bart\n",
    "    chunk_end += tokenizer.model_max_length  # == 1024 for Bart\n",
    "\n",
    "# generate a summary on each batch\n",
    "summary_ids_lst = [model.generate(inputs, num_beams=4, max_length=100, early_stopping=True) for inputs in inputs_batch_lst]\n",
    "\n",
    "# decode the output and join into one string with one paragraph per summary batch\n",
    "summary_batch_lst = []\n",
    "for summary_id in summary_ids_lst:\n",
    "    summary_batch = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_id]\n",
    "    summary_batch_lst.append(summary_batch[0])\n",
    "summary_all = '\\n'.join(summary_batch_lst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbc201-2626-4efd-acc3-d3a1f6980a48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
